{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd0b0285039934b2c45504955ea823010382f959d5d9978cc1be0d4284abc2126ee",
   "display_name": "Python 3.9.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "b0285039934b2c45504955ea823010382f959d5d9978cc1be0d4284abc2126ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = os.path.abspath('')\n",
    "data = np.genfromtxt('./dataR2.csv', delimiter=',', skip_header=True)\n",
    "# mieszam sobie zbiór, bo... bo mogę\n",
    "np.random.shuffle(data)\n",
    "# height to de facto liczba badań\n",
    "height, _ = data.shape\n",
    "# liczba zbiorów testowych, liczba zbiorów treningowych\n",
    "# potem się okaże, że wystarczy jedno z nich, ale nie chce mi się już używać korektora\n",
    "n_test, n_train = round(0.2*height), height-round(0.2*height)\n",
    "# sprawdzam, czy nie potknąłem się o zaokrąglenie, ale pewnie jakby wziąć pod uwagę dokładność numeryczną, to poniższy krok prawdopodobnie nie ma sensu\n",
    "# (sztuka dla sztuki, jak paw dumnie prezentuje swój ogon, tak jak dumnie prezentuję znajomość niesamowicie skomplikowanej składni języka Python)\n",
    "# <owacje>\n",
    "assert n_test+n_train==height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove broken data (needles if skip_header=True)\n",
    "filtered = []\n",
    "for row in data:\n",
    "    if np.isnan(np.sum(row)):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(93, 9) (93,)\n(93, 10) (23, 10) 93 23\n"
     ]
    }
   ],
   "source": [
    "train_set = data[:n_train,:]\n",
    "train_x, train_y = train_set[:,:-1], train_set[:,-1:].squeeze()\n",
    "print(train_x.shape, train_y.shape)\n",
    "test_set=data[n_train:,:]\n",
    "test_x, test_y = test_set[:,:-1], test_set[:,-1:].squeeze()\n",
    "print(train_set.shape, test_set.shape, n_train, n_test)\n",
    "total_x, total_y = data[:,:-1], data[:,-1:].squeeze()\n",
    "# nie ma potrzeby normalizacji train_y i test_y do {0,1}, ponieważ macierz predykcji wskazuje na prawdopodobieństwo wystąpienia kolejnych klas\n",
    "# (w naszym przypadku to 1 i 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=0, ).fit(train_x, train_y)\n",
    "# podobno liblinear is good for small datasets, a ja nie mam zamiaru się z tym kłucić"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 2.]\n              precision    recall  f1-score   support\n\n         1.0       0.71      0.92      0.80        13\n         2.0       0.83      0.50      0.62        10\n\n    accuracy                           0.74        23\n   macro avg       0.77      0.71      0.71        23\nweighted avg       0.76      0.74      0.72        23\n\n***\nOdp.: Trafność decyzji wynosi 0.713, a czułość modelu 0.712.\n"
     ]
    }
   ],
   "source": [
    "print(model.classes_)\n",
    "# ważne!\n",
    "# w naszym przypadku są dwie klasy (liczba kolumn w macierzy predict_proba)\n",
    "# model.predict bierze pod uwagę macierz n x 2, gdzie pierwsza kolumna to prawdopodobieństwo pierwszej klasy (1),\n",
    "# a druga kolumna to prawd drugiej klasy (2). Czyli (wg mnie) próg odcięcia tutaj wynosi 0.5\n",
    "pred_y = model.predict(test_x)\n",
    "cls_report = classification_report(test_y, pred_y, output_dict=False)\n",
    "cls_report_dict = classification_report(test_y, pred_y, output_dict=True)\n",
    "print(cls_report)\n",
    "trafnosc_predykcji = cls_report_dict.keys()\n",
    "trafnosc_decyzji = round(cls_report_dict['macro avg']['f1-score'], 3)\n",
    "# podobno recall to jest czułość\n",
    "czulosc = round(cls_report_dict['macro avg']['recall'], 3)\n",
    "print('***')\n",
    "print(f'Odp.: Trafność decyzji wynosi {trafnosc_decyzji}, a czułość modelu {czulosc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********\n",
    "# podpunkt B\n",
    "# **********\n",
    "\n",
    "COST_TP = 48_477\n",
    "COST_TN = 0\n",
    "COST_FP = 751\n",
    "COST_FN = 89_463"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_py(model, p, t):\n",
    "    return [model.classes_[1] if pi[1] > t else model.classes_[0] for pi in p]\n",
    "\n",
    "def optimize(model, x, y):\n",
    "    # FUNKCJA OPTYMALIZUJĄCA (podpunkt b)\n",
    "    # dostajemy nauczony model oraz testowe x i y (na podstawie których będziemy optymalizować)\n",
    "    p = model.predict_proba(x)\n",
    "    t0 = 0.5\n",
    "    def fun(t):\n",
    "        # print(t)\n",
    "        # poniżej uzależniamy predykcję od punktu odcięcia t\n",
    "        py = get_py(model, p, t)\n",
    "        cm = confusion_matrix(y, py)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        return tn * COST_TN + fp * COST_FP + fn * COST_FN + tp * COST_TP\n",
    "        # print(py)\n",
    "        # print(model.predict(x))\n",
    "    # cm = confusion_matrix(y, py)\n",
    "    # print(cm)\n",
    "    res = minimize(fun, t0, method='nelder-mead', options={'xatol': 1e-8, 'disp': True})\n",
    "    return res.x[0]\n",
    "\n",
    "# optimize(model, test_x, test_y)\n",
    "# print(total_x.shape, total_y.shape)\n",
    "# optimize(model, total_x, total_y)\n",
    "# print(test_y)\n",
    "# print(model.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- tylko dla danych testowych ---\nOptimization terminated successfully.\n         Current function value: 650216.000000\n         Iterations: 24\n         Function evaluations: 69\noptymalny punkt odciecia = 0.475\n[2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0]\n              precision    recall  f1-score   support\n\n         1.0       0.73      0.85      0.79        13\n         2.0       0.75      0.60      0.67        10\n\n    accuracy                           0.74        23\n   macro avg       0.74      0.72      0.73        23\nweighted avg       0.74      0.74      0.73        23\n\n***\nOdp.: Trafność decyzji wynosi 0.726, a czułość modelu 0.723.\n"
     ]
    }
   ],
   "source": [
    "print('--- tylko dla danych testowych (podpunkt c) ---')\n",
    "t_opt = round(optimize(model, test_x, test_y), 3)\n",
    "print(f'optymalny punkt odciecia = {t_opt}')\n",
    "opt_y = get_py(model, model.predict_proba(test_x), t_opt)\n",
    "print(opt_y)\n",
    "cls_report = classification_report(test_y, opt_y, output_dict=False)\n",
    "cls_report_dict = classification_report(test_y, opt_y, output_dict=True)\n",
    "print(cls_report)\n",
    "trafnosc_predykcji = cls_report_dict.keys()\n",
    "trafnosc_decyzji = round(cls_report_dict['macro avg']['f1-score'], 3)\n",
    "# podobno recall to jest czułość\n",
    "czulosc = round(cls_report_dict['macro avg']['recall'], 3)\n",
    "print('***')\n",
    "print(f'Odp.: Trafność decyzji wynosi {trafnosc_decyzji}, a czułość modelu {czulosc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- dla całego zbioru danych ---\nOptimization terminated successfully.\n         Current function value: 3137074.000000\n         Iterations: 32\n         Function evaluations: 86\noptymalny punkt odciecia = 0.1\n[2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0]\n              precision    recall  f1-score   support\n\n         1.0       1.00      0.12      0.21        52\n         2.0       0.58      1.00      0.74        64\n\n    accuracy                           0.60       116\n   macro avg       0.79      0.56      0.47       116\nweighted avg       0.77      0.60      0.50       116\n\n***\nOdp.: Trafność decyzji wynosi 0.471, a czułość modelu 0.558.\n"
     ]
    }
   ],
   "source": [
    "print('--- dla całego zbioru danych (podpunkt c) ---')\n",
    "t_opt = round(optimize(model, total_x, total_y), 3)\n",
    "print(f'optymalny punkt odciecia = {t_opt}')\n",
    "opt_y = get_py(model, model.predict_proba(total_x), t_opt)\n",
    "print(opt_y)\n",
    "cls_report = classification_report(total_y, opt_y, output_dict=False)\n",
    "cls_report_dict = classification_report(total_y, opt_y, output_dict=True)\n",
    "print(cls_report)\n",
    "trafnosc_predykcji = cls_report_dict.keys()\n",
    "trafnosc_decyzji = round(cls_report_dict['macro avg']['f1-score'], 3)\n",
    "# podobno recall to jest czułość\n",
    "czulosc = round(cls_report_dict['macro avg']['recall'], 3)\n",
    "print('***')\n",
    "print(f'Odp.: Trafność decyzji wynosi {trafnosc_decyzji}, a czułość modelu {czulosc}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nmoja obserwacja:\nFinansowo optymalny punkt odcięcia to 0.1. Pomimo, że model wówczas jest mało precyzyjny, to ma to sens, gdyż lepiej (finansowo) jest więcej osób sklasyfikować jako chore i ew poświęcić 751 kosztu niż błędnie klasyfikować więcej osób jako zdrowe i poświęcać potem dwukrony koszt leczenie. Zdaje się to całkiem intuicyjne.\n---\nwytłumaczenie:\nPowyżej optymalizowałem tylko dla danych testowych i widać, że punkt odcięcia poszedł trochę w dół, ale porządna optymalizacja wyszła dopiero dla wszystkich danych.\n\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "moja obserwacja:\n",
    "Finansowo optymalny punkt odcięcia to 0.1. Pomimo, że model wówczas jest mało precyzyjny, to ma to sens, gdyż lepiej (finansowo) jest więcej osób sklasyfikować jako chore i ew poświęcić 751 kosztu niż błędnie klasyfikować więcej osób jako zdrowe i poświęcać potem dwukrony koszt leczenie. Zdaje się to całkiem intuicyjne.\n",
    "---\n",
    "wytłumaczenie:\n",
    "Powyżej optymalizowałem tylko dla danych testowych i widać, że punkt odcięcia poszedł trochę w dół, ale porządna optymalizacja wyszła dopiero dla wszystkich danych.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- dla całego zbioru danych (podpunkt c) ---\nOptimization terminated successfully.\n         Current function value: 24.000000\n         Iterations: 24\n         Function evaluations: 68\noptymalny punkt odciecia = 0.497\n[1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n              precision    recall  f1-score   support\n\n         1.0       0.75      0.81      0.78        52\n         2.0       0.83      0.78      0.81        64\n\n    accuracy                           0.79       116\n   macro avg       0.79      0.79      0.79       116\nweighted avg       0.80      0.79      0.79       116\n\n***\nOdp.: Trafność decyzji wynosi 0.792, a czułość modelu 0.794.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "dla porównania zmodyfikowaliśmy funkcję optimize tak, żeby przyjmowała koszty=0 dla dobrych predykcji i 1 dla złych\n",
    "(chodzi o to, żeby tylko w przypadku złej decyzji trzeba było płacić)\n",
    "teraz widać, że punkt odcięcia zmienia się tak, żeby model był jak najbardziej skuteczny\n",
    "\"\"\"\n",
    "\n",
    "def optimize2(model, x, y):\n",
    "    # FUNKCJA OPTYMALIZUJĄCA (podpunkt b)\n",
    "    # dostajemy nauczony model oraz testowe x i y (na podstawie których będziemy optymalizować)\n",
    "    p = model.predict_proba(x)\n",
    "    t0 = 0.5\n",
    "    def fun(t):\n",
    "        # poniżej uzależniamy predykcję od punktu odcięcia t\n",
    "        py = get_py(model, p, t)\n",
    "        cm = confusion_matrix(y, py)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        return tn * 0 + fp * 1 + fn * 1 + tp * 0\n",
    "        # print(py)\n",
    "        # print(model.predict(x))\n",
    "    # cm = confusion_matrix(y, py)\n",
    "    # print(cm)\n",
    "    res = minimize(fun, t0, method='nelder-mead', options={'xatol': 1e-8, 'disp': True})\n",
    "    return res.x[0]\n",
    "\n",
    "print('--- dla całego zbioru danych (podpunkt c) ---')\n",
    "t_opt = round(optimize2(model, total_x, total_y), 3)\n",
    "print(f'optymalny punkt odciecia = {t_opt}')\n",
    "opt_y = get_py(model, model.predict_proba(total_x), t_opt)\n",
    "print(opt_y)\n",
    "cls_report = classification_report(total_y, opt_y, output_dict=False)\n",
    "cls_report_dict = classification_report(total_y, opt_y, output_dict=True)\n",
    "print(cls_report)\n",
    "trafnosc_predykcji = cls_report_dict.keys()\n",
    "trafnosc_decyzji = round(cls_report_dict['macro avg']['f1-score'], 3)\n",
    "# podobno recall to jest czułość\n",
    "czulosc = round(cls_report_dict['macro avg']['recall'], 3)\n",
    "print('***')\n",
    "print(f'Odp.: Trafność decyzji wynosi {trafnosc_decyzji}, a czułość modelu {czulosc}.')"
   ]
  }
 ]
}